"""
Title        :  cris.py
What is it   :  Routines to prepare input, train, and predict CrIS ML models
Includes     :  features_l1b()
                                prediction()
Author        : Frank Werner
Date          : 20240723
Modf          : 20240912: Fixed '\n' in features_l1b()
                20241002: Added dtype to features_l1b
                20250204: Added temp_prior and h2o_prior to features_l1b()
                20250207: Added rad_mw option to features_l1b()

"""

# Import modules
# =======================================
import numpy as np
import glob
import h5py
from scipy.stats import pearsonr

from .rmsd_two_var import rmsd_two_var

from tensorflow.keras.models import load_model
from sklearn.metrics import (
    confusion_matrix,
    accuracy_score,
    f1_score,
    precision_score,
    matthews_corrcoef,
)
import os
from pathlib import Path

# Functions and classes
# =======================================
def features_l1b(
    l1b=None, prior=None, temp_prior=None, h2o_prior=None, ml_model_path : str | os.PathLike[str]=""
):
    ##############
    # This reshapes CrIS L1B data, as generated by read_cris_l1b.py.
    #
    # Parameters
    # ---------
    # l1b: object; L1B data from read_cris_l1b.py
    # prior, temp_prior, h2o_prior: ndarrays; species, temperature, and h2o priors
    # ml_model_path: string; full path to the ML model
    #
    # Returns
    # -------
    # features : ndarray; [n_obs x n_vars] features
    ##############

    # Define variables
    n_files = len(l1b.granule)
    n_obs = n_files * 45 * 30 * 9

    # Slices for rad_mw
    scaling_file = next(Path(ml_model_path).glob("*scaling*_new.h5"))
    if "NH3" in str(scaling_file):
        slice_freq = slice(502, 512 + 1)

    # Reshape data
    fov = np.reshape(l1b.fov, (n_obs))
    rad_lw = np.reshape(l1b.rad_lw, (n_obs, 717))
    rad_mw = np.reshape(l1b.rad_mw, (n_obs, 869))
    rad_sw = np.reshape(l1b.rad_sw, (n_obs, 637))
    latitude = np.reshape(l1b.latitude, (n_obs))
    longitude = np.reshape(l1b.longitude, (n_obs))
    utc = np.reshape(l1b.utc, (n_obs))
    sol_zen_ang = np.reshape(l1b.sol_zen_ang, (n_obs))
    day_night_flag = np.reshape(l1b.day_night_flag, (n_obs))
    surf_alt = np.reshape(l1b.surf_alt, (n_obs))
    view_ang = np.reshape(l1b.view_ang, (n_obs))
    doy = np.reshape(l1b.doy, (n_obs))

    # Reshape apriori, if available
    if prior is not None:
        # This is not properly implemented yet
        apriori = np.reshape(prior, (n_obs, len(prior[0, 0, 0, 0, :])))
    else:
        apriori = None
    if temp_prior is not None:
        # This is not properly implemented yet
        temp_apriori = np.reshape(temp_prior, (n_obs, len(temp_prior[0, 0, 0, 0, :])))
    else:
        temp_apriori = None
    if h2o_prior is not None:
        # This is not properly implemented yet
        h2o_apriori = np.reshape(h2o_prior, (n_obs, len(h2o_prior[0, 0, 0, 0, :])))
    else:
        h2o_apriori = None

    # Generate a combined radiance variable
    rad_dummy = np.append(rad_lw, rad_mw, axis=1)
    rad_dummy = np.append(rad_dummy, rad_sw, axis=1)
    rad = np.reshape(rad_dummy, (n_obs, 2223))

    # Read features order
    with open(Path(ml_model_path) / "features_order.txt", "r") as f:
        features_order = f.read()
    features_order = features_order.split(" ")
    # features_order[-1] = features_order[-1][0:-1]
    if "\n" in features_order[-1]:
        features_order[-1] = features_order[-1][0:-1]

    # Count the number of features
    n_vars = 0
    for var in features_order:
        if var == "rad":
            n_vars += len(rad[0, :])
        if var == "rad_mw":
            n_vars += len(rad[0, slice_freq])
        if var == "fov":
            n_vars += 1
        if var == "latitude":
            n_vars += 1
        if var == "longitude":
            n_vars += 1
        if var == "utc":
            n_vars += 1
        if var == "sol_zen_ang":
            n_vars += 1
        if var == "day_night_flag":
            n_vars += 1
        if var == "view_ang":
            n_vars += 1
        if var == "doy":
            n_vars += 1
        if var == "colprior" or var == "prior":
            n_vars += len(apriori[0, :])
        if var == "temp_prior":
            n_vars += len(temp_apriori[0, :])
        if var == "h2o_prior":
            n_vars += len(h2o_apriori[0, :])
        if var == "surf_alt":
            n_vars += 1

    # Generate features
    features = np.zeros((len(latitude), n_vars), dtype=np.float32)
    n_vars = 0
    for var in features_order:
        if var == "rad":
            features[:, n_vars : n_vars + len(rad[0, :])] = rad
            n_vars += len(rad[0, :])
        if var == "rad_mw":
            features[:, n_vars : n_vars + len(rad[0, slice_freq])] = rad[:, slice_freq]
            n_vars += len(rad[0, slice_freq])
        if var == "fov":
            features[:, n_vars] = fov
            n_vars += 1
        if var == "latitude":
            features[:, n_vars] = latitude
            n_vars += 1
        if var == "longitude":
            features[:, n_vars] = longitude
            n_vars += 1
        if var == "utc":
            features[:, n_vars] = utc
            n_vars += 1
        if var == "sol_zen_ang":
            features[:, n_vars] = sol_zen_ang
            n_vars += 1
        if var == "day_night_flag":
            features[:, n_vars] = day_night_flag
            n_vars += 1
        if var == "view_ang":
            features[:, n_vars] = view_ang
            n_vars += 1
        if var == "doy":
            features[:, n_vars] = doy
            n_vars += 1
        if var == "colprior" or var == "prior":
            features[:, n_vars : n_vars + len(apriori[0, :])] = apriori
            n_vars += len(apriori[0, :])
        if var == "temp_prior":
            features[:, n_vars : n_vars + len(temp_apriori[0, :])] = temp_apriori
            n_vars += len(temp_apriori[0, :])
        if var == "h2o_prior":
            features[:, n_vars : n_vars + len(h2o_apriori[0, :])] = h2o_apriori
            n_vars += len(h2o_apriori[0, :])
        if var == "surf_alt":
            features[:, n_vars] = surf_alt
            n_vars += 1

    # Return features
    return features


class __PredictionClass:
    ##############
    # This creates a class for the predictions from a Keras multi-layer
    #                   perceptron. You can find these attributes below or
    #                   by typing: print(self.__dict__.keys()).
    #
    # Parameters
    # ---------
    # var1, var2, var3, var4, var5, var6, var7, var8, var9, var10: ndarrays;
    #                   type, labels, predictions, mask, and evaluation
    #                   from prediction().
    #
    # Returns
    # -------
    # self : object/class; an object with the original and predicted labels,
    #                   outlier mask, evaluation metrics, and the scaling
    #                   factor for the data (e.g., 1e6 for ppmv; for the
    #                   rmsd calculation and plotting routines).
    ##############
    def __init__(self, var1, var2, var3, var4, var5, var6, var7, var8, var9, var10):
        if var1 == "regression":
            self.labels = var2
            self.labels_pred = var3
            self.labels_uncert = var4
            self.quality_flag = var5
            self.corr_coeff = var6
            self.rmsd = var7
            self.nrmsd = var8
            self.perc_diff = var9
            self.scale_fac = var10
        if var1 == "classification":
            self.labels = var2
            self.labels_pred = var3
            self.labels_uncert = var4
            self.quality_flag = var5
            self.cm = var6
            self.acc = var7
            self.f1 = var8
            self.prec = var9
            self.mcc = var10


def prediction(
    mdl=None,
    mdl_api="sequential",
    mdl_type="regression",
    categorial=False,
    load_weights=False,
    load_fullmodel=True,
    path : str | os.PathLike[str] ="",
    prefix="",
    suffix="",
    extra_suffix="",
    features=None,
    labels=None,
    enc_features=False,
    enc_features_path="",
    enc_features_prefix="",
    enc_features_suffix="",
    enc_labels=False,
    enc_labels_path="",
    enc_labels_prefix="",
    enc_labels_suffix="",
    features_transform="standardize",
    labels_transform="standardize",
    standardize_features=True,
    renorm_labels=False,
    renorm_pred=True,
    labels_log_scale=False,
    labels_negexp_scale=False,
    batch_size_in=32,
    scale_fac=1.0,
    scale_fac_conv=1.0,
    ensemble=False,
    prob_labels=False,
    n_pred=10,
    flag_outliers=True,
    evaluate=True,
    save_evaluate=True,
):
    ##############
    # Predict with a Keras multi-layer perceptron.
    #
    # Parameters
    # ---------
    # mdl: Keras model; a previously set-up Keras MLP.
    # mdl_api (optional): string; structure of ann. The default value is
    #                   'sequential' and can be changed to 'functional',
    #                   'functional_multi_input', or 'subclass'.
    #                   Reshaping of functional and subclass labels is needed.
    # mdl_type (optional): string; type of ann. The default value is
    #                   'regression' and can be changed to 'classification'.
    # categorial (optional): boolean; if set to True, the predicted labels are
    #                   transformed from binary class matrices to class vectors.
    # load_weights (optional): boolean; if set to True, weights are loaded.
    # load_fullmodel (optional): boolean; if set to True, a model is loaded.
    # path', prefix, suffix, extra_suffix (optional): strings;
    #                   building blocks to generate the path and filename of
    #                   the files that contain the weights.
    #                   The extra_suffix keyword can be used to save specific
    #                   files for e.g., 'training' and 'test' data.
    # features: ndarray; features (i.e., input) for which we want a prediction.
    # labels (optional): original labels (i.e., output); only used if we
    #                   want to evaluate the predictions.
    # enc_features: boolean; whether to encode the input with existing model.
    # enc_features_path', enc_features_prefix, enc_features_suffix: strings;
    #                   building blocks to generate the path and filename
    #                   of the files that contain the encoder indices.
    # enc_labels: boolean; whether to encode the labels with existing model.
    # enc_labels_path', enc_labels_prefix, enc_labels_suffix: strings;
    #                   building blocks to generate the path and filename
    #                   of the files that contain the encoder indices.
    # features_transform (optional): string; 'standardize' or 'normalize'
    #                   features (i.e., input). Default is 'standardize'.
    # labels_transform (optional): string; 'standardize' or 'normalize'
    #                   labels (i.e., output). Default is 'standardize'.
    # standardize_features (optional): boolean; if set to True, the features
    #                   are standardized. Otherwise, it is assumed that they
    #                   are already standardized.
    #                   Note that while it is called 'standardize', it can likewise
    #                   be normalized, depending on 'features_transform'.
    # renorm_labels (optional): boolean; If set to True, the labels are
    #                   renormalized (i.e., it is assumed the input is normalized).
    #                   Needed for evaluations of the predictions.
    #                   Note that while it is called 'renorm', it can likewise
    #                   be restandardized, depending on 'labels_transform'.
    # renorm_pred (optional): boolean; If set to True, the predicted labels are
    #                   renormalized (i.e., it is assumed the training was performed
    #                   on normalized data).
    #                   Note that while it is called 'renorm', it can likewise
    #                   be restandardized, depending on 'labels_transform'.
    # labels_log_scale (optional): boolean; if set to True, the labels are
    #                   tranformed to sinh(labels), i.e., the labels were
    #                   arcsinh(labels) before training.
    # labels_negexp_scale (optional): boolean; if set to True, the labels are
    #                   tranformed to -1.*np.log(labels), i.e., the labels were
    #                   np.exp(-labels) before training.
    # batch_size_in (optional): integer; mini-batch size. 1 is purely
    #                   stochastic. Should be powers of 2 (i.e., 2**7 = 128)
    #                   for memorey purposes.
    # scale_fac (optional): float or double; scale factor applied to the labels.
    #                   This shoud be the one used in input_data().
    #                   Needed for labels_log_scale and labels_negexp_scale.
    # scale_fac_conv (optional): float or double; scale factor for convenience.
    #                   Useful for precise rmsd calculations,
    #                   and subsequent plotting routines.
    #                   Note that scale_fac is needed to get the right scaling;
    #                   this one is just for convenience and might be different.
    # ensemble (optional): boolean; specify whether permanent dropout layers
    #                   or DenseVariational layers are in the model.
    #                   If True, we will perform multiple predictions.
    # prob_labels (optional): boolean; specifies whether the labels are
    #                   deterministic or probabilistic. This also affects
    #                   the predictions.
    # n_pred (optional): integer; number of predictions.
    # flag_outliers (optional): boolean; if set to True, unreliable predictions
    #                   are flagged based on the training range.
    # evaluate (optional): boolean; if set to True, the labels are used to
    #                   evaluate the predictions.
    # save_evaluate (optional): boolean; if set to True, the evaluation
    #                   metrics are saved.
    #
    # Returns
    # -------
    # data_out: __PredictionClass() class/object which contains the
    #                   predictions (and original labels), evaluation metrics,
    #                   and the scaling factor.
    ##############

    # 1) Some housekeeping
    ##########################################################################
    file_in_weights = path / f"{prefix}_weights_{suffix}.keras"
    file_in_scale = path / f"{prefix}_scaling_{suffix}.h5"
    if extra_suffix != "":
        file_out_evaluate = (
            path / f"{prefix}_evaluation_{suffix}{extra_suffix}.h5"
        )
    else:
        file_out_evaluate = path / f"{prefix}_evaluation_{suffix}.h5"

    # Transpose the labels when using functional API
    if mdl_api == "functional" or mdl_api == "subclass":
        labels = np.transpose(labels)

    # Encoder models
    if enc_features is True:
        file_in_enc_features_weights = (
            enc_features_path
            + enc_features_prefix
            + "_weights_"
            + enc_features_suffix
            + ".tf"
        )
    if enc_labels is True:
        file_in_enc_labels_weights = (
            enc_labels_path
            + enc_labels_prefix
            + "_weights_"
            + enc_labels_suffix
            + ".tf"
        )

    # 2) Get model and scaling information, scale features
    # and labels (latter only needed for evaluation)
    ##########################################################################

    # Load the model weights, if necessary
    if load_weights is True:
        mdl.load_weights(file_in_weights)
    if load_fullmodel is True:
        mdl = load_model(file_in_weights)

    # Read scaling factors
    file = h5py.File(file_in_scale, "r")
    features_mean = file["features_mean"][:]
    features_std = file["features_std"][:]
    if "features_min" in file.keys():
        features_min = file["features_min"][:]
        features_max = file["features_max"][:]
    labels_mean = file["labels_mean"][:]
    labels_std = file["labels_std"][:]
    if "labels_min" in file.keys():
        labels_min = file["labels_min"][:]
        labels_max = file["labels_max"][:]
    if "labels_addval" in file.keys():
        labels_addval = file["labels_addval"][:]
    else:
        labels_addval = None
    file.close()

    # Fiddle with log labels, exp- labels, added value, and scale factor
    if type(labels_log_scale) is bool:
        if categorial is False:
            labels_log_scale_val = np.zeros((len(labels_mean)), dtype=bool)
        else:
            labels_log_scale_val = np.zeros((len(labels_mean) + 1), dtype=bool)
        labels_log_scale_val[:] = labels_log_scale
    else:
        labels_log_scale_val = np.ravel(np.array(labels_log_scale))
    if type(labels_negexp_scale) is bool:
        if categorial is False:
            labels_negexp_scale_val = np.zeros((len(labels_mean)), dtype=bool)
        else:
            labels_negexp_scale_val = np.zeros((len(labels_mean) + 1), dtype=bool)
        labels_negexp_scale_val[:] = labels_negexp_scale
    else:
        labels_negexp_scale_val = np.ravel(np.array(labels_negexp_scale))
    if labels_addval is not None:
        if (
            type(labels_addval) is int
            or type(labels_addval) is float
            or type(labels_addval) is np.float64
        ):
            if categorial is False:
                labels_addval_val = np.zeros((len(labels_mean)))
            else:
                labels_addval_val = np.zeros((len(labels_mean) + 1))
            labels_addval_val[:] = labels_addval
        else:
            labels_addval_val = np.ravel(np.array(labels_addval))
    if scale_fac is not None:
        if (
            type(scale_fac) is int
            or type(scale_fac) is float
            or type(scale_fac) is np.float64
        ):
            scale_fac_val = np.zeros((len(labels_mean)))
            scale_fac_val[:] = scale_fac
        else:
            scale_fac_val = np.ravel(np.array(scale_fac))

    # Normalize features, if necessary
    if standardize_features is True:
        if features_transform == "standardize":
            features = (features - features_mean) / features_std
        if features_transform == "normalize":
            features = (features - features_min) / (features_max - features_min)

    # Encode the features, if necessary
    if enc_features is True:
        enc_features_mdl = load_model(file_in_enc_features_weights)
        features = enc_features_mdl.encoder(features)

    # Decode the labels, if necessary
    if enc_labels is True:
        enc_labels_mdl = load_model(file_in_enc_labels_weights)
        labels = np.array(enc_labels_mdl.decoder(labels))

    # Transform the labels, if necessary
    if (
        np.ravel(features)[0] is not None
        and renorm_labels is True
        and categorial is False
    ):
        # Renormalize predictions
        if labels_transform == "standardize":
            labels = labels * labels_std + labels_mean
        if labels_transform == "normalize":
            labels = labels * (labels_max - labels_min) + labels_min
        # Deal with possible log labels
        for i_lab in range(0, len(labels[0, :])):
            if labels_log_scale_val[i_lab]:
                labels[:, i_lab] = np.sinh(labels[:, i_lab])
        # Deal with possible exp- labels
        for i_lab in range(0, len(labels[0, :])):
            if labels_negexp_scale_val[i_lab]:
                labels[:, i_lab] = -1 * np.log(labels[:, i_lab])
        # Remove added value
        if labels_addval is not None:
            labels -= labels_addval_val
        # Apply scale factor
        if scale_fac is not None and type(labels) is np.ndarray:
            # if type(labels).__module__ == np.__name__:
            labels *= scale_fac_val

    # 3) Predict
    ##########################################################################
    if np.ravel(features)[0] is not None:
        if ensemble is False:
            if prob_labels is False:
                # Predict labels
                if mdl_api == "sequential" or mdl_api == "functional_multi_input":
                    labels_pred = mdl.predict(features, batch_size=batch_size_in)
                if mdl_api == "functional":
                    labels_pred = np.transpose(
                        np.squeeze(mdl.predict([features], batch_size=batch_size_in))
                    )
                if mdl_api == "subclass":
                    labels_pred_d = mdl.predict(features, batch_size=batch_size_in)
                    labels_pred = np.zeros((len(features), len(labels[0, :])))
                    count_key = 0
                    for key in labels_pred_d.keys():
                        labels_pred[:, count_key] = labels_pred_d[key][:, 0]
                        count_key += 1
                # Decode the predictions
                if enc_labels:
                    labels_pred = np.array(enc_labels_mdl.decoder(labels_pred))
                # Renormalize predictions
                if renorm_pred:
                    if labels_transform == "standardize":
                        labels_pred = labels_pred * labels_std + labels_mean
                    if labels_transform == "normalize":
                        labels_pred = (
                            labels_pred * (labels_max - labels_min) + labels_min
                        )
                # Deal with possible log labels
                for i_lab in range(0, len(labels_pred[0, :])):
                    if labels_log_scale_val[i_lab]:
                        labels_pred[:, i_lab] = np.sinh(labels_pred[:, i_lab])
                # Deal with possible exp- labels
                for i_lab in range(0, len(labels_pred[0, :])):
                    if labels_negexp_scale_val[i_lab]:
                        labels_pred[:, i_lab] = -1 * np.log(labels_pred[:, i_lab])
                # Remove added value
                if labels_addval is not None:
                    labels_pred -= labels_addval_val
                # Apply scale factor
                if scale_fac is not None:
                    labels_pred *= scale_fac_val
                # Uncertainty
                labels_uncert = None
            else:
                # Predict distribution
                prediction_distribution = mdl(features)
                # Mean and standard deviation of the distribution
                labels_pred = prediction_distribution.mean().numpy()
                labels_pred_stdv = prediction_distribution.stddev().numpy()
                # Renormalize predictions
                if renorm_pred:
                    if labels_transform == "standardize" and categorial is False:
                        labels_pred = labels_pred * labels_std + labels_mean
                        labels_pred_stdv = labels_pred_stdv * labels_std + labels_mean
                    if labels_transform == "normalize" and categorial is False:
                        labels_pred = (
                            labels_pred * (labels_max - labels_min) + labels_min
                        )
                        labels_pred_stdv = (
                            labels_pred_stdv * (labels_max - labels_min) + labels_min
                        )
                # Deal with possible log labels
                for i_lab in range(0, len(labels_pred[0, :])):
                    if labels_log_scale_val[i_lab]:
                        labels_pred[:, i_lab] = np.sinh(labels_pred[:, i_lab])
                        labels_pred_stdv[:, i_lab] = np.sinh(labels_pred_stdv[:, i_lab])
                # Deal with possible exp- labels
                for i_lab in range(0, len(labels_pred[0, :])):
                    if labels_negexp_scale_val[i_lab]:
                        labels_pred[:, i_lab] = -1 * np.log(labels_pred[:, i_lab])
                        labels_pred_stdv[:, i_lab] = -1 * np.log(
                            labels_pred_stdv[:, i_lab]
                        )
                # Uncertainty
                labels_uncert = labels_pred_stdv
                # Remove added value
                if labels_addval is not None:
                    labels_pred -= labels_addval_val
                # Apply scale factor
                if scale_fac is not None:
                    labels_pred *= scale_fac_val
                    labels_uncert *= scale_fac_val
        else:
            if prob_labels is False:
                labels_pred_dummy = []
                for i_pred in range(0, n_pred):
                    # Predict labels
                    dummy = mdl.predict(features)
                    # Renormalize predictions
                    if renorm_pred:
                        if labels_transform == "standardize" and categorial is False:
                            dummy = dummy * labels_std + labels_mean
                        if labels_transform == "normalize" and categorial is False:
                            dummy = dummy * (labels_max - labels_min) + labels_min
                    # Deal with possible log labels
                    for i_lab in range(0, len(dummy[0, :])):
                        if labels_log_scale_val[i_lab]:
                            dummy[:, i_lab] = np.sinh(dummy[:, i_lab])
                    # Deal with possible exp- labels
                    for i_lab in range(0, len(dummy[0, :])):
                        if labels_negexp_scale_val[i_lab]:
                            dummy[:, i_lab] = -1 * np.log(dummy[:, i_lab])
                    # Remove added value
                    if labels_addval is not None:
                        dummy -= labels_addval
                    # Apply scale factor
                    if scale_fac is not None:
                        dummy *= scale_fac_val
                    # Append
                    labels_pred_dummy.append(dummy)
                labels_pred = np.nanmean(labels_pred_dummy, axis=0)
                labels_uncert = np.nanstd(labels_pred_dummy, axis=0)  # epistemic
            else:
                labels_pred_dummy = []
                labels_uncert_dummy = []
                for i_pred in range(0, n_pred):
                    # Predict distribution
                    prediction_distribution = mdl(features)
                    # Mean and standard deviation of the distribution
                    dummy_mean = prediction_distribution.mean().numpy()
                    dummy_stdv = prediction_distribution.stddev().numpy()
                    # Renormalize predictions
                    if renorm_pred:
                        if labels_transform == "standardize" and categorial is False:
                            dummy_mean = dummy_mean * labels_std + labels_mean
                            dummy_stdv = dummy_stdv * labels_std + labels_mean
                        if labels_transform == "normalize" and categorial is False:
                            dummy_mean = (
                                dummy_mean * (labels_max - labels_min) + labels_min
                            )
                            dummy_stdv = (
                                dummy_stdv * (labels_max - labels_min) + labels_min
                            )
                    # Deal with possible log labels
                    for i_lab in range(0, len(dummy_mean[0, :])):
                        if labels_log_scale_val[i_lab]:
                            dummy_mean[:, i_lab] = np.sinh(dummy_mean[:, i_lab])
                            dummy_stdv[:, i_lab] = np.sinh(dummy_stdv[:, i_lab])
                    # Deal with possible exp- labels
                    for i_lab in range(0, len(dummy_mean[0, :])):
                        if labels_negexp_scale_val[i_lab]:
                            dummy_mean[:, i_lab] = -1 * np.log(dummy_mean[:, i_lab])
                            dummy_stdv[:, i_lab] = -1 * np.log(dummy_stdv[:, i_lab])
                    # Remove added value
                    if labels_addval is not None:
                        dummy_mean -= labels_addval_val
                    # Apply scale factor
                    if scale_fac is not None:
                        dummy_mean *= scale_fac_val
                        dummy_stdv *= scale_fac_val
                    # Append
                    labels_pred_dummy.append(dummy_mean)
                    labels_uncert_dummy.append(dummy_stdv)
                labels_pred = np.mean(labels_pred_dummy, axis=0)
                labels_uncert_epistemic = np.std(labels_pred_dummy, axis=0)
                labels_uncert_aleatoric = np.mean(labels_uncert_dummy, axis=0)
                labels_uncert = labels_uncert_epistemic + labels_uncert_aleatoric
    else:
        labels_pred = None
        labels_uncert = None

    # Deal with categorial data
    if categorial is True:
        if labels is not None:
            labels_dummy = np.zeros((len(labels_pred)), dtype=int)

            for i in range(0, len(labels)):
                labels_dummy[i] = int(np.argmax(labels[i]))

            labels = labels_dummy

        labels_pred_dummy = np.zeros((len(labels_pred)), dtype=int)

        for i in range(0, len(labels_pred)):
            labels_pred_dummy[i] = int(np.argmax(labels_pred[i]))

        labels_pred = labels_pred_dummy

    # 4) Flag for quality. We'll allow for some extrapolation.
    # 1.0: everything is good
    # 0.8: there are features outside the training range, but within the extended range
    # 0.6: there are labels outside the training range, but within the extended range
    # 0.5: there are features and labels outside the training range, but within the extended range
    # 0.4: there are features outside the extended training range
    # 0.2: there are labels outside the extended training range
    # 0.0: there are features and labels outside the extended training range
    ##########################################################################
    if flag_outliers is True:
        if categorial is False:
            # --- Prepare quality_flag ---
            quality_flag = np.zeros_like(labels_pred)

            # --- Normalize/standardize feature ranges ---
            if features_transform == "standardize":
                features_min_scaled = (features_min - features_mean) / features_std
                features_max_scaled = (features_max - features_mean) / features_std
            elif features_transform == "normalize":
                features_min_scaled = 0 * features_min
                features_max_scaled = 1 * features_max
            else:
                features_min_scaled = features_min
                features_max_scaled = features_max

            features_spread_scaled = features_max_scaled - features_min_scaled
            features_min_ext_scaled = features_min_scaled - 0.1 * features_spread_scaled
            features_max_ext_scaled = features_max_scaled + 0.1 * features_spread_scaled

            # --- Apply possible log-scaling ---
            labels_min_scaled = np.copy(labels_min)
            labels_max_scaled = np.copy(labels_max)
            for i_lab in range(0, len(labels_mean)):
                if labels_log_scale_val[i_lab]:
                    labels_min_scaled[i_lab] = np.sinh(labels_min[i_lab])
                    labels_max_scaled[i_lab] = np.sinh(labels_max[i_lab])

            # --- Feature and label range checks with epsilon tolerance ---
            EPS = 1e-6
            feat_out_main = np.logical_or(
                features < features_min_scaled - EPS,
                features > features_max_scaled + EPS,
            )
            feat_out_ext = np.logical_or(
                features < features_min_ext_scaled - EPS,
                features > features_max_ext_scaled + EPS,
            )

            lbl_out_main = np.logical_or(
                labels_pred < labels_min_scaled - EPS,
                labels_pred > labels_max_scaled + EPS,
            )
            lbl_out_ext = np.logical_or(
                labels_pred
                < (labels_min_scaled - 0.1 * (labels_max_scaled - labels_min_scaled))
                - EPS,
                labels_pred
                > (labels_max_scaled + 0.1 * (labels_max_scaled - labels_min_scaled))
                + EPS,
            )

            feature_flag = np.where(
                feat_out_ext.any(axis=1), 2, np.where(feat_out_main.any(axis=1), 1, 0)
            )
            label_flag = np.where(lbl_out_ext, 2, np.where(lbl_out_main, 1, 0))

            feature_flag_expand = np.repeat(
                feature_flag[:, np.newaxis], labels_pred.shape[1], axis=1
            )

            # --- Assign quality flag ---
            quality_flag = np.where(
                (feature_flag_expand == 0) & (label_flag == 0),
                1.0,
                np.where(
                    (feature_flag_expand == 1) & (label_flag == 0),
                    0.8,
                    np.where(
                        (feature_flag_expand == 0) & (label_flag == 1),
                        0.6,
                        np.where(
                            (feature_flag_expand == 1) & (label_flag == 1),
                            0.5,
                            np.where(
                                (feature_flag_expand == 2) & (label_flag == 1),
                                0.4,
                                np.where(
                                    (feature_flag_expand == 1) & (label_flag == 2),
                                    0.2,
                                    np.where(
                                        (feature_flag_expand == 2) & (label_flag == 2),
                                        0.0,
                                        1.0,  # fallback
                                    ),
                                ),
                            ),
                        ),
                    ),
                ),
            )

        else:
            # Categorical case
            quality_flag = np.zeros(len(features))

            # Scale features_min and features_max
            if features_transform == "standardize":
                features_min_scaled = (features_min - features_mean) / features_std
                features_max_scaled = (features_max - features_mean) / features_std
            elif features_transform == "normalize":
                features_min_scaled = (features_min - features_min) / (
                    features_max - features_min
                )
                features_max_scaled = (features_max - features_min) / (
                    features_max - features_min
                )
            else:
                features_min_scaled = features_min
                features_max_scaled = features_max

            # Compute extended feature min/max in scaled space
            features_spread_scaled = features_max_scaled - features_min_scaled
            features_min_ext_scaled = features_min_scaled - 0.1 * features_spread_scaled
            features_max_ext_scaled = features_max_scaled + 0.1 * features_spread_scaled

            # Go through each observation
            for i_obs in range(len(features)):
                feat = features[i_obs, :]
                feat_out_main = np.logical_or(
                    feat < features_min_scaled, feat > features_max_scaled
                )
                feat_out_ext = np.logical_or(
                    feat < features_min_ext_scaled, feat > features_max_ext_scaled
                )

                if feat_out_main.any():
                    quality_flag[i_obs] = 0 if feat_out_ext.any() else 0.5
                elif feat_out_ext.any():
                    quality_flag[i_obs] = 0.5
                else:
                    quality_flag[i_obs] = 1

    else:
        # No flagging => all good quality
        quality_flag = np.ones((len(labels_pred)))

    # 5) Evaluate
    ##########################################################################
    if mdl_type == "regression":
        if evaluate is True:
            # Calculate corr_coeff, rmsd, nrmsd, and perc_diff
            n_labels = len(labels_pred[0, :])

            corr_coeff = np.zeros((n_labels))
            rmsd = np.zeros((n_labels))
            nrmsd = np.zeros((n_labels))
            perc_diff = np.zeros((3, n_labels))

            # Note that there is no falesafe here.
            # The program will fail if any of the input is None. That is intended.
            for i in range(0, n_labels):
                ind = np.where(
                    (np.isfinite(labels[:, i])) & (np.isfinite(labels_pred[:, i]))
                )[0]
                corr_coeff[i] = pearsonr(
                    labels[ind, i] * scale_fac_conv,
                    labels_pred[ind, i] * scale_fac_conv,
                )[0]
                rmsd[i] = rmsd_two_var(
                    labels[ind, i] * scale_fac_conv,
                    labels_pred[ind, i] * scale_fac_conv,
                )
                nrmsd[i] = rmsd_two_var(
                    labels[ind, i] * scale_fac_conv,
                    labels_pred[ind, i] * scale_fac_conv,
                ) / (np.mean(labels[ind, i]) * scale_fac_conv)
                perc_diff[0, i] = np.nanpercentile(
                    labels_pred[ind, i] * scale_fac_conv
                    - labels[ind, i] * scale_fac_conv,
                    1,
                )
                perc_diff[1, i] = np.nanpercentile(
                    labels_pred[ind, i] * scale_fac_conv
                    - labels[ind, i] * scale_fac_conv,
                    50,
                )
                perc_diff[2, i] = np.nanpercentile(
                    labels_pred[ind, i] * scale_fac_conv
                    - labels[ind, i] * scale_fac_conv,
                    99,
                )

            # Save metrics to file
            if save_evaluate is True:
                file = h5py.File(file_out_evaluate, "w")
                file.create_dataset(
                    "corr_coeff",
                    data=corr_coeff,
                    shape=[n_labels],
                    dtype=corr_coeff.dtype,
                )
                file.create_dataset(
                    "rmsd", data=rmsd, shape=[n_labels], dtype=rmsd.dtype
                )
                file.create_dataset(
                    "nrmsd", data=nrmsd, shape=[n_labels], dtype=nrmsd.dtype
                )
                file.create_dataset(
                    "perc_diff",
                    data=perc_diff,
                    shape=[3, n_labels],
                    dtype=perc_diff.dtype,
                )
                file.create_dataset(
                    "scale_fac", data=scale_fac_conv, shape=[1], dtype=np.float32
                )
                file.close()
        else:
            corr_coeff = None
            rmsd = None
            nrmsd = None
            perc_diff = None

    if mdl_type == "classification":
        if evaluate is True:
            if ensemble is True:
                # Calculate confusion matrix
                conf_mat = confusion_matrix(labels[:,], np.round(labels_pred[:,]))

                # Calculate a number of classification metrics
                acc = accuracy_score(labels[:,], np.round(labels_pred[:,]))
                f1 = f1_score(labels[:,], np.round(labels_pred[:,]), average="weighted")
                prec = precision_score(
                    labels[:,], np.round(labels_pred[:,]), average="weighted"
                )
                mcc = matthews_corrcoef(labels[:,], np.round(labels_pred[:,]))
            else:
                # Calculate confusion matrix
                conf_mat = confusion_matrix(labels[:,], labels_pred[:,])

                # Calculate a number of classification metrics
                acc = accuracy_score(labels[:,], labels_pred[:,])
                f1 = f1_score(labels[:,], labels_pred[:,], average="weighted")
                prec = precision_score(labels[:,], labels_pred[:,], average="weighted")
                mcc = matthews_corrcoef(labels[:,], labels_pred[:,])

            # Save metrics to file
            if save_evaluate is True:
                file = h5py.File(file_out_evaluate, "w")
                file.create_dataset(
                    "conf_mat",
                    data=conf_mat,
                    shape=[len(conf_mat[:, 0]), len(conf_mat[0, :])],
                    dtype=conf_mat.dtype,
                )
                file.create_dataset(
                    "acc", data=acc, shape=[1], dtype=np.array([acc]).dtype
                )
                file.create_dataset(
                    "f1", data=f1, shape=[1], dtype=np.array([f1]).dtype
                )
                file.create_dataset(
                    "prec", data=prec, shape=[1], dtype=np.array([prec]).dtype
                )
                file.create_dataset(
                    "mcc", data=mcc, shape=[1], dtype=np.array([mcc]).dtype
                )
                file.close()
        else:
            conf_mat = None
            acc = None
            f1 = None
            prec = None
            mcc = None

    # 6) Fill an object with all these variables and return data_out
    ##########################################################################
    if mdl_type == "regression":
        data_out = __PredictionClass(
            mdl_type,
            labels,
            labels_pred,
            labels_uncert,
            quality_flag,
            corr_coeff,
            rmsd,
            nrmsd,
            perc_diff,
            scale_fac_conv,
        )
    if mdl_type == "classification":
        data_out = __PredictionClass(
            mdl_type,
            labels,
            labels_pred,
            labels_uncert,
            quality_flag,
            conf_mat,
            acc,
            f1,
            prec,
            mcc,
        )

    # Return data_out
    return data_out


__all__ = ["features_l1b", "prediction"]
